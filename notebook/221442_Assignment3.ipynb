{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab6f363-ad25-4f35-99af-e33fe0eaf12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Models will be saved to: D:\\Assignment3\\Models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import string\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "DATASET_DIR = r\"D:\\Assignment3\\Dataset\"\n",
    "IMAGE_FOLDER = os.path.join(DATASET_DIR, \"Images\")\n",
    "CAPTIONS_FILE = os.path.join(DATASET_DIR, \"captions.txt\")\n",
    "\n",
    "MODEL_SAVE_PATH = r\"D:\\Assignment3\\Models\"\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "print(f\"Models will be saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86cf12f-b502-4c37-a171-f8e625a62e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def tokenizer_eng(self, text):\n",
    "        # Efficient pure-python tokenizer\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        return text.split()\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "        return [self.stoi[\"<SOS>\"]] + \\\n",
    "               [self.stoi.get(word, self.stoi[\"<UNK>\"]) for word in tokenized_text] + \\\n",
    "               [self.stoi[\"<EOS>\"]]\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=2):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = self._read_captions(captions_file)\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary([x[1] for x in self.df])\n",
    "\n",
    "    def _read_captions(self, captions_file):\n",
    "        data = []\n",
    "        with open(captions_file, \"r\", encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader) \n",
    "            for line in reader:\n",
    "                if len(line) >= 2:\n",
    "                    data.append((line[0], line[1]))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.df[index][1]\n",
    "        img_id = self.df[index][0]\n",
    "        try:\n",
    "            img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            return self.__getitem__((index + 1) % len(self.df))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption)\n",
    "\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
    "        return imgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6afa719c-bd04-41f5-a751-ea578e8a8d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset (this may take a minute)...\n",
      "Dataset Loaded! Vocab size: 5224\n"
     ]
    }
   ],
   "source": [
    "# Stronger transforms for training\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((232, 232)),\n",
    "    transforms.RandomCrop((224, 224)),    \n",
    "    transforms.RandomHorizontalFlip(),     \n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "print(\"Loading dataset (this may take a minute)...\")\n",
    "dataset = FlickrDataset(\n",
    "    root_dir=IMAGE_FOLDER,\n",
    "    captions_file=CAPTIONS_FILE,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=0,  \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=MyCollate(pad_idx=pad_idx)\n",
    ")\n",
    "\n",
    "print(f\"Dataset Loaded! Vocab size: {len(dataset.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08aec070-3497-405b-a062-241e6d5e8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.inception = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.inception(images)\n",
    "        return self.dropout(self.relu(features))\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.3) \n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions[:, :-1])\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, max_len=20):\n",
    "        # Inference function\n",
    "        result_caption = []\n",
    "        with torch.no_grad():\n",
    "            inputs = features.unsqueeze(1)\n",
    "            states = None\n",
    "            for _ in range(max_len):\n",
    "                hiddens, states = self.lstm(inputs, states)\n",
    "                output = self.linear(hiddens.squeeze(1))\n",
    "                predicted = output.argmax(1)\n",
    "                \n",
    "                result_caption.append(predicted.item())\n",
    "                \n",
    "                inputs = self.embed(predicted).unsqueeze(1) \n",
    "                \n",
    "                if predicted.item() == 2: # <EOS>\n",
    "                    break\n",
    "        return result_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c993c5-f2d5-494d-af32-5e92cd3dcb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xpg\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to D:\\Assignment3\\Models\\vocab.pkl\n",
      "Starting Training...\n",
      "Epoch [1/15] Step [0/1265] Loss: 8.5595\n",
      "Epoch [1/15] Step [100/1265] Loss: 4.0770\n",
      "Epoch [1/15] Step [200/1265] Loss: 3.8040\n",
      "Epoch [1/15] Step [300/1265] Loss: 3.5874\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "num_layers = 2\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 15\n",
    "\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(\n",
    "    list(decoder.parameters()) + list(encoder.parameters()), \n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
    "\n",
    "vocab_path = os.path.join(MODEL_SAVE_PATH, \"vocab.pkl\")\n",
    "with open(vocab_path, \"wb\") as f:\n",
    "    pickle.dump(dataset.vocab, f)\n",
    "print(f\"Vocabulary saved to {vocab_path}\")\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx, (imgs, captions) in enumerate(loader):\n",
    "        imgs = imgs.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        features = encoder(imgs)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Step [{idx}/{len(loader)}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    scheduler.step(avg_loss) \n",
    "    print(f\"Epoch {epoch+1} Completed. Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save Model Checkpoints\n",
    "    if (epoch+1) % 5 == 0 or epoch == num_epochs-1:\n",
    "        enc_path = os.path.join(MODEL_SAVE_PATH, \"encoder.pth\")\n",
    "        dec_path = os.path.join(MODEL_SAVE_PATH, \"decoder.pth\")\n",
    "        \n",
    "        torch.save(encoder.state_dict(), enc_path)\n",
    "        torch.save(decoder.state_dict(), dec_path)\n",
    "        print(f\"Saved models to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "print(\"Training Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea249fdf-32f7-47ec-a09a-5e10b2613d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import traceback\n",
    "\n",
    "# Load models just to be safe (ensure we use the saved ones)\n",
    "# encoder.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, \"encoder.pth\")))\n",
    "# decoder.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, \"decoder.pth\")))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Simple transform for testing\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "def get_caption_from_model(image, encoder, decoder, vocab, max_len=20):\n",
    "    image_tensor = test_transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = encoder(image_tensor)\n",
    "        output_ids = decoder.sample(features, max_len=max_len)\n",
    "    \n",
    "    caption_words = []\n",
    "    for word_id in output_ids:\n",
    "        word = vocab.itos[word_id]\n",
    "        if word == \"<EOS>\":\n",
    "            break\n",
    "        if word not in [\"<SOS>\", \"<PAD>\", \"<UNK>\"]:\n",
    "            caption_words.append(word)\n",
    "            \n",
    "    return \" \".join(caption_words)\n",
    "\n",
    "# Upload Widget\n",
    "uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_upload_change(change):\n",
    "    if not uploader.value: return\n",
    "    \n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"Processing...\")\n",
    "    \n",
    "    try:\n",
    "        # Handle ipywidgets version differences\n",
    "        if isinstance(uploader.value, dict):\n",
    "            uploaded_file = list(uploader.value.values())[0]\n",
    "        else:\n",
    "            uploaded_file = list(uploader.value)[-1]\n",
    "            \n",
    "        image = Image.open(io.BytesIO(uploaded_file['content'])).convert(\"RGB\")\n",
    "        \n",
    "        # Generate\n",
    "        caption = get_caption_from_model(image, encoder, decoder, dataset.vocab)\n",
    "        \n",
    "        with output:\n",
    "            clear_output()\n",
    "            display(image.resize((300, 300)))\n",
    "            print(f\"\\nGenerated Caption: {caption}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        with output:\n",
    "            print(\"Error:\", traceback.format_exc())\n",
    "\n",
    "uploader.observe(on_upload_change, names='value')\n",
    "print(\"Upload an image below to test:\")\n",
    "display(uploader, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
